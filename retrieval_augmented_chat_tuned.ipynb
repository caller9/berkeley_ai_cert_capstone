{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da28ba98-c899-441a-9ea6-a78a7c7c9716",
   "metadata": {},
   "source": [
    "# Retrieval augmented chat - fine tuned\n",
    "\n",
    "This notebook is the primary demonstration of the project with the fine tuned model. Here we'll bring up the tuned model and vector database and start asking questions both with and without the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea286197-5a87-4f1b-abc3-bef2298a37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99fd829-3019-4de8-b27c-947c8a7f2d9b",
   "metadata": {},
   "source": [
    "## Initialize some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a05ed5f-66ef-4cc5-9813-736405bdefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"Corpus\"\n",
    "merged_model_dir = \"merged-fine-tuned\"\n",
    "device_map = {\"\": 0}\n",
    "device = \"cuda\"\n",
    "database_top_n_results = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035b376-3b51-4da9-a4fd-23f2a7d2f5fc",
   "metadata": {},
   "source": [
    "## Load the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c10f83e-c140-4e54-8dd9-be410440bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"db/\")\n",
    "collection = client.get_collection(name = collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde5370-9aa9-4138-8e0e-a52e17c0f6db",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a41d92-5a28-4668-8e8b-2f37bf0180e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0734da983c04c60b53b2ddb2eb93e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_model_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179fa16b-0172-45a1-b237-0d702a00e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_model(msg):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": msg},\n",
    "    ]\n",
    "    \n",
    "    encoded = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = merged_model.generate(encoded, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id, temperature=0.8)\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "def send_chat(msg):\n",
    "    result = send_to_model(msg)[0]\n",
    "    return result.rsplit(\" [/INST] \", 1)[1]\n",
    "\n",
    "def basic_chat(msg):\n",
    "    print(send_chat(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c18cc-2fd8-46b5-a107-1d21c494187a",
   "metadata": {},
   "source": [
    "## Trying the model without access to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551660eb-3f81-434e-8029-f0babd9b7969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To open a GIF in a loop in Python, use this code: \n",
      "\n",
      "```\n",
      "import gifsicle\n",
      "\n",
      "with open('file.gif', 'rb') as f:\n",
      "    gif = gifsicle.GIF(f)\n",
      "\n",
      "    for i in range(0, len(gif), 1):\n",
      "        gif.display()\n",
      "        gif.sleep()\n",
      "```\n",
      "\n",
      "This will open the GIF file in a loop, displaying each frame of the GIF for a set length of time determined by the `sleep()` method. The `range()` function specifies the number of times to loop through the file, starting at 0 and ending at the length of the GIF minus 1.\n",
      "\n",
      "To make the loop indefinite, use this code:\n",
      "\n",
      "```\n",
      "import gifsicle\n",
      "\n",
      "with open('file.gif', 'rb') as f:\n",
      "    gif = gifsicle.GIF(f)\n",
      "\n",
      "    while True:\n",
      "        gif.display()\n",
      "        gif.sleep()\n",
      "```\n",
      "\n",
      "This will keep the GIF looping indefinitely, displaying each frame until the program is closed.\n"
     ]
    }
   ],
   "source": [
    "basic_chat(\"How do I loop a GIF?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0a712-94f8-4ed2-9049-9eb2df8bb674",
   "metadata": {},
   "source": [
    "## Add some retrieval convenience methods\n",
    "These methods use the vector database to find the `database_top_n_results` from the vector database, add them into the request context, then annotate the result with links to the documents used in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b632cef2-dec6-4f72-89e8-d09ee214dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "def retrieval_augmented_chat(msg):\n",
    "    query_result = collection.query(\n",
    "        query_texts=[msg], \n",
    "        n_results=database_top_n_results\n",
    "    )\n",
    "    question_with_context = \"\"\n",
    "    if len(query_result['documents'][0]) > 0:\n",
    "        question_with_context = \"Based on the following documents:\\n\" + \"\\n\\n\".join(query_result['documents'][0]) + \"\\n Answer the following question with lots of details: \"\n",
    "    question_with_context += msg\n",
    "    model_response = send_chat(question_with_context)\n",
    "\n",
    "    doc_links = \"\"\n",
    "    if len(query_result['metadatas'][0]) > 0:\n",
    "        doc_links = \"\\n\\n **Reference documents:** \\n\\n\"\n",
    "        for i in range(0, len(query_result['metadatas'][0])):\n",
    "            source = query_result['metadatas'][0][i]['source']\n",
    "            score = query_result['distances'][0][i]\n",
    "            doc_links += f\"* [{source}]({source}) score: {score:3.2f}\\n\"\n",
    "    return model_response + doc_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476ee89-d947-44e2-9c82-6797396cd3b2",
   "metadata": {},
   "source": [
    "## Run retrieval augmented chat\n",
    "Notice that the responses have switched from a general HTML/JavaScript context to include the document contents which provide examples of encoding the gif with ImageMagick so that it loops by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b0024a-5c2d-42bf-bbba-3533b3cf2fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To make a GIF loop in ImageMagick, follow these steps:\n",
       "\n",
       "1. Open a terminal window and navigate to the directory where the GIF file is saved.\n",
       "2. Use the following command to open the GIF file in ImageMagick:\n",
       "```\n",
       "convert chrome-samesite-missing.gif\n",
       "```\n",
       "Replace `chrome-samesite-missing.gif` with the name of your GIF file.\n",
       "\n",
       "3. Use the following command to make the GIF loop indefinitely:\n",
       "```\n",
       "-loop 0\n",
       "```\n",
       "This tells the GIF to loop indefinitely until the user stops it.\n",
       "\n",
       "4. Use the following command to save the looping GIF:\n",
       "```\n",
       "chrome-samesite-missing-loop.gif\n",
       "```\n",
       "This saves the looping GIF with the same name as the original GIF, but with the word \"loop\" added to the end.\n",
       "\n",
       "The complete command to loop a GIF in ImageMagick is:\n",
       "```\n",
       "convert chrome-samesite-missing.gif -loop 0 chrome-samesite-missing-loop.gif\n",
       "```\n",
       "Note that the output filename comes last, AFTER the `-loop 0` option.\n",
       "\n",
       " **Reference documents:** \n",
       "\n",
       "* [corpus/imagemagick/set-a-gif-to-loop.md](corpus/imagemagick/set-a-gif-to-loop.md) score: 0.97\n",
       "* [corpus/imagemagick/compress-animated-gif.md](corpus/imagemagick/compress-animated-gif.md) score: 1.28\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(retrieval_augmented_chat(\"How do I loop a GIF?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbd178ea-8f8d-4a0f-a4ce-9bcde8dffd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, you can use npx with GitHub actions. To do this, you will first need to install the required package globally using npm. For example, if you want to use the `npm install` command to install the required package globally, you can use the following code in your GitHub action workflow:\n",
       "```yaml\n",
       "- name: Install npm globally\n",
       "  run: npm install -g npm\n",
       "```\n",
       "Once you have installed the required package globally, you can use the `npx` command in your workflow to execute the package. For example, if the package you want to use is called `my-package`, and it has a binary called `my-binary`, you can use the following code in your workflow:\n",
       "```yaml\n",
       "- name: Run my-binary using npx\n",
       "  run: npx my-package/my-binary\n",
       "```\n",
       "This will install the required package using the globally installed npm, and then execute the `my-binary` binary from the package.\n",
       "\n",
       "I hope this helps! Let me know if you have any other questions.\n",
       "\n",
       " **Reference documents:** \n",
       "\n",
       "* [corpus/github-actions/npm-cache-with-npx-no-package.md](corpus/github-actions/npm-cache-with-npx-no-package.md) score: 0.88\n",
       "* [corpus/github-actions/attach-generated-file-to-release.md](corpus/github-actions/attach-generated-file-to-release.md) score: 1.11\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(retrieval_augmented_chat(\"Can I use npx with GitHub actions?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3ed6c-25c5-43b4-a3fb-b7dc52dba8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
