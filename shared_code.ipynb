{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4756cc84-ec9f-4f13-9395-cae957ace139",
   "metadata": {},
   "source": [
    "# Shared Code\n",
    "This file contains shared classes used across the various retrieval_augmented_chat notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e86be9b-be4f-4393-9e48-cc32e886df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db53913f-3093-47f1-a0b3-c1fc568bb28c",
   "metadata": {},
   "source": [
    "## Interaction with a local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c849f5b6-1ebc-4cfc-b622-cfac2665eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModel:\n",
    "    def __init__(self, model, tokenizer, inst_separator = \" [/INST] \", temperature = 0.4):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inst_separator = inst_separator\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def __send_to_model(self, msg):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": msg},\n",
    "        ]\n",
    "        \n",
    "        encoded = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        generated_ids = self.model.generate(encoded, max_new_tokens=1000, do_sample=True, pad_token_id=self.tokenizer.eos_token_id, temperature=0.8, repetition_penalty=1.20)\n",
    "        decoded = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        return decoded\n",
    "\n",
    "    def send_chat(self, msg):\n",
    "        result = self.__send_to_model(msg)[0]\n",
    "        return result.rsplit(self.inst_separator, 1)[1]\n",
    "\n",
    "    def basic_chat(self, msg):\n",
    "        print(self.send_chat(msg))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b399c-25a2-4eef-8d21-9de9c72a1c9b",
   "metadata": {},
   "source": [
    "## Interaction with OpenAI API\n",
    "Here we subclass ChatModel and override the functions to use their API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c60bb7-e0be-4381-91f7-7e80f6d68911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAiChatModel(ChatModel):\n",
    "    def __init__(self, organization, api_key, openai_model, temperature = 0.4):\n",
    "        # The base class expects a model and tokenizer, and we don't have them.\n",
    "        ChatModel.__init__(self, None, None) \n",
    "        self.openai_client = OpenAI(\n",
    "            organization=openai_organization,\n",
    "            api_key=openai_api_key\n",
    "        )\n",
    "        self.openai_model = openai_model\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __send_to_model(self, msg):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": msg},\n",
    "        ]\n",
    "    \n",
    "        completion = self.openai_client.chat.completions.create(\n",
    "            model = self.openai_model,\n",
    "            messages = messages,\n",
    "            temperature = self.temperature\n",
    "        )\n",
    "    \n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    def send_chat(self, msg):\n",
    "        return self.__send_to_model(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfd372-93f9-4d29-a0b6-318167a0ff06",
   "metadata": {},
   "source": [
    "## Retrieval convenience methods\n",
    "These methods use the vector database to find the `database_top_n_results` from the vector database, add them into the request context, then annotate the result with links to the documents used in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2525f5b8-1d1b-4b47-9b7f-6a88a28ecc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAugmentedChat:\n",
    "    def __init__(self, path, collection_name, top_n_results, chat_model):\n",
    "        self.client = chromadb.PersistentClient(path=path)\n",
    "        self.collection = self.client.get_collection(name = collection_name)\n",
    "        self.top_n_results = top_n_results\n",
    "        self.chat_model = chat_model\n",
    "    \n",
    "    def printmd(self, string):\n",
    "        display(Markdown(string))\n",
    "    \n",
    "    def chat(self, msg):\n",
    "        query_result = self.collection.query(\n",
    "            query_texts=[msg], \n",
    "            n_results=database_top_n_results\n",
    "        )\n",
    "        question_with_context = \"\"\n",
    "        if len(query_result['documents'][0]) > 0:\n",
    "            question_with_context = \"Based on the following documents:\\n\" + \"\\n\\n\".join(query_result['documents'][0]) + \"\\n Answer the following question with lots of details: \"\n",
    "        question_with_context += msg\n",
    "        start = timer()\n",
    "        model_response = self.chat_model.send_chat(question_with_context)\n",
    "        end = timer()\n",
    "\n",
    "        model_response_time = f\"\\n**Inference time in seconds {end - start:3.4f}**\\n\"\n",
    "    \n",
    "        doc_links = \"\"\n",
    "        if len(query_result['metadatas'][0]) > 0:\n",
    "            doc_links = \"\\n\\n **Reference documents:** \\n\\n\"\n",
    "            for i in range(0, len(query_result['metadatas'][0])):\n",
    "                source = query_result['metadatas'][0][i]['source']\n",
    "                distance = query_result['distances'][0][i]\n",
    "                doc_links += f\"* [{source}]({source}) distance: {distance:3.2f}\\n\"\n",
    "        return model_response + doc_links + model_response_time\n",
    "\n",
    "    def markdown_chat(self, msg):\n",
    "        self.printmd(self.chat(msg))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
