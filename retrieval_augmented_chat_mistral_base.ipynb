{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da28ba98-c899-441a-9ea6-a78a7c7c9716",
   "metadata": {},
   "source": [
    "# Retrieval augmented chat - Mistral-7B\n",
    "\n",
    "This notebook is the primary demonstration of the project with the baseline model. It has not been instruction tuned. Here we'll bring up the baseline model and vector database and start asking questions both with and without the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea286197-5a87-4f1b-abc3-bef2298a37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99fd829-3019-4de8-b27c-947c8a7f2d9b",
   "metadata": {},
   "source": [
    "## Initialize some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a05ed5f-66ef-4cc5-9813-736405bdefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"Corpus\"\n",
    "model_dir = \"mistralai/Mistral-7B-v0.1\"\n",
    "device_map = {\"\": 0}\n",
    "device = \"cuda\"\n",
    "database_top_n_results = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a9d7d-1e12-4e0e-a308-50dd0725428b",
   "metadata": {},
   "source": [
    "## Load shared code\n",
    "This file defines the `ChatModel` and `Retrieval` classes used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19aff328-00b9-4971-bfaa-ff9b579c1938",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run shared_code.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde5370-9aa9-4138-8e0e-a52e17c0f6db",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a41d92-5a28-4668-8e8b-2f37bf0180e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90233f3312c94318a392f89041fec99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "language_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633c44f-acce-4ad5-9ded-434fe3f88b0d",
   "metadata": {},
   "source": [
    "## Load the model into the ChatModel class from `shared_code.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804c5685-8021-4a4e-a055-392d9dca611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a different instruction separator because the untuned\n",
    "# model formats the output incorrectly.\n",
    "chat_model = ChatModel(language_model, tokenizer, \"[/INST]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c18cc-2fd8-46b5-a107-1d21c494187a",
   "metadata": {},
   "source": [
    "## Trying the model without access to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551660eb-3f81-434e-8029-f0babd9b7969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To stop a gif from looping, you would use the following code:\n",
      "\n",
      "```\n",
      "<img src=\"http://example.com/myimage.gif\" width=\"123\" height=\"456\" loop=\"-1\"/>\n",
      "```\n",
      "\n",
      "The “loop” attribute is set to “-1”, which will stop the gif from looping.\n"
     ]
    }
   ],
   "source": [
    "chat_model.basic_chat(\"How do I loop a GIF?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fdc2dc-5fc0-49c0-8ca9-adb3fa5da4a2",
   "metadata": {},
   "source": [
    "## Load the collection into the RetrievalAugmentedChat class from `shared_code.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0748c25-95f1-45cd-a547-03eaa212f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "rac = RetrievalAugmentedChat(\"db/\", collection_name, database_top_n_results, chat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476ee89-d947-44e2-9c82-6797396cd3b2",
   "metadata": {},
   "source": [
    "## Run retrieval augmented chat\n",
    "Notice that the responses are empty. In several iterations with the model it either doesn't run the inference very long or just spits out an identical copy of the input without summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b0024a-5c2d-42bf-bbba-3533b3cf2fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       " **Reference documents:** \n",
       "\n",
       "* [corpus/imagemagick/set-a-gif-to-loop.md](corpus/imagemagick/set-a-gif-to-loop.md) distance: 0.97\n",
       "* [corpus/imagemagick/compress-animated-gif.md](corpus/imagemagick/compress-animated-gif.md) distance: 1.28\n",
       "\n",
       "**Inference time in seconds 0.2102**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rac.markdown_chat(\"How do I loop a GIF?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbd178ea-8f8d-4a0f-a4ce-9bcde8dffd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       " **Reference documents:** \n",
       "\n",
       "* [corpus/github-actions/npm-cache-with-npx-no-package.md](corpus/github-actions/npm-cache-with-npx-no-package.md) distance: 0.88\n",
       "* [corpus/github-actions/attach-generated-file-to-release.md](corpus/github-actions/attach-generated-file-to-release.md) distance: 1.11\n",
       "\n",
       "**Inference time in seconds 0.1733**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rac.markdown_chat(\"Can I use npx with GitHub actions?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3ed6c-25c5-43b4-a3fb-b7dc52dba8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
