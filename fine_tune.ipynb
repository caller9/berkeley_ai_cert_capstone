{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b35880-2789-4697-870b-2434551af7dd",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "This notebook will use the `data/fine_tune.jsonl` file to fine tune the raw Mistral 7B baseline model to perform better at Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f286eb2a-31ac-4421-a710-2f94dc2625e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 22:09:53.067489: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-13 22:09:53.090744: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 22:09:53.625619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24d4d9-f3f9-4f02-9523-762ecd37979c",
   "metadata": {},
   "source": [
    "## Configuring parameters for training\n",
    "Parts of this process are based on a combination of [Fine-Tune Your Own Llama 2 Model in a Colab Notebook - Maxime Labonne](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32) \n",
    "and [A Beginner’s Guide to Fine-Tuning Mistral 7B Instruct Model - Adithya S K](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe) with some fine tuning of parameters to run on a single 24GB VRAM Nvidia Cuda compatible GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e094088b-a0c9-4ff5-a2a7-317478044522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# LoRa\n",
    "new_model_dir = \"mistralai-lora\"\n",
    "\n",
    "# Merged model\n",
    "merged_model_dir = \"merged-fine-tuned\"\n",
    "\n",
    "# Fine tuning data path\n",
    "fine_tune_file = \"data/fine_tune.jsonl\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs if not -1)\n",
    "max_steps = -1\n",
    "\n",
    "# Enable fp16/bf16 training (bf16 set to True automatically below if supported)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 1000\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 100\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use. If you encounter an out of memory error during\n",
    "# training you can tune this value to find the maximum. However, don't set this\n",
    "# value while training or it will truncate your training data.\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# define the device type\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea7007-b4b2-4600-beaf-7e3a48da2c3c",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a458f392-742b-47d2-b2ee-faa57b767626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerating training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f00bd294384b84aa581b32c752b240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerating training with bf16=True\")\n",
    "        bf16 = True\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb372dc3-d870-4945-b70c-fa56cf1bcf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d26a0-64b4-41d2-9ebe-06cab24e5e2e",
   "metadata": {},
   "source": [
    "## Test out the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98ea8549-b0de-4678-bde3-3335d3fad8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Omit this step to save VRAM for training. Uncomment to test.\n",
    "# messages = [ \n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "# model_inputs = encodeds.to(device)\n",
    "\n",
    "# generated_ids = base_model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "# decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "# print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69007e89-5211-4568-b6c7-6b1c550793a7",
   "metadata": {},
   "source": [
    "## Load the training dataset\n",
    "\n",
    "The training dataset consists of entries formatted as recommended by [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format) in JSON Lines `jsonl` format.\n",
    "\n",
    "Example:\n",
    "```\n",
    "{text: \"<s>[INST] Something the user says [/INST] Desirable response from the model.</s>\"}\n",
    "{text: \"<s>[INST] Another thing from the user [/INST] Another response from the model.</s>\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f7782f-da0a-48c1-a280-3a364b136149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab046e88fe76421caf143f44af90cc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88395eba9a34dc1af1c4668b4b0e8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fcdba8c7ed4845a5c239cb177d8fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset('json', data_files=fine_tune_file , split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874316a5-d8f8-46d6-9aaa-ad03a7977d91",
   "metadata": {},
   "source": [
    "## Configure and run fine tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c7a3e5-d952-4bb8-80e6-417e88f7be71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pwnt/jupyter/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:173: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341c43939d6342ad9a2bfcaff76068cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6cd63-72c1-4264-bb31-61bcbfc9fac9",
   "metadata": {},
   "source": [
    "## Run training\n",
    "This is the most time consuming part of the operation. It took 3 hours and 35 minutes on a RTX4090 with an AMD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec676e89-3079-4a0e-99a0-b3ebb14cf79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26037' max='26037' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26037/26037 3:27:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.320500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.311300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.380900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.251500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.331300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.283900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.363400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.402100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.354800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.436100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.853900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.848600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.945500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.965800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.891200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.959600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.966900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.961600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.962300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.960100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.098800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>1.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>1.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>1.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>1.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>1.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>1.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>1.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>1.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>1.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>1.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>1.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>1.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>1.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>1.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>1.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>1.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>1.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>1.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>1.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>1.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>1.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>1.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>1.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>1.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>1.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>1.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>1.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>1.202600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>1.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>1.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>1.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>1.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.718600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.723400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.758900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.851700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.788500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.815400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.796100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.814400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.823900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.820800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.893600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.851700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.874100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.804400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.934800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.934100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.895500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.898100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.946200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.929400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.873300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.894400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.883500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.943300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.905700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>1.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.986200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>1.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.987100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>1.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>1.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.966800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>1.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>1.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>1.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.987300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>1.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>1.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>1.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.971600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>1.100900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>1.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>1.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>1.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>1.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>1.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>1.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.029300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee55ca-63db-4de0-9b85-dbc95e762528",
   "metadata": {},
   "source": [
    "## Free up VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b8e09c-c22f-4b62-9c11-813138e4d54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41334"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del base_model\n",
    "gc.collect()\n",
    "\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850969f9-28af-414f-a3cd-c39f71113ed8",
   "metadata": {},
   "source": [
    "Sometimes you'll need to run this cache clearing step multiple times if the following \"Save the merged model\" step causes an out of memory (OOM) error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3eb143-50b7-4b37-a1aa-a3758df69e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab255dd1-8362-4eb3-8e1b-d63464a10ccc",
   "metadata": {},
   "source": [
    "## Save the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83eaaff5-6a18-4fa3-a6ad-3559ee28e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b58976831546cd8a086fc5d21afc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, new_model_dir)\n",
    "merged_model = merged_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d22961-218c-4333-82a2-35336aa325e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged model\n",
    "merged_model.save_pretrained(merged_model_dir,safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db30f0-3c2c-4666-a170-9143a23dec4a",
   "metadata": {},
   "source": [
    "## Test out the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91523e06-d678-4ca6-9d19-f809095cf375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_model(model, msg):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": msg},\n",
    "    ]\n",
    "    \n",
    "    encoded = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(encoded, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id, temperature=0.4, repetition_penalty=1.20)\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return decoded\n",
    "    \n",
    "def send_chat(model, msg):\n",
    "    result = send_to_model(model, msg)[0]\n",
    "    return result.rsplit(\" [/INST] \", 1)[1]\n",
    "\n",
    "def print_chat(model, msg):\n",
    "    print(send_chat(model, msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0bfe170-99fb-454b-8307-04d1f6699c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a bulleted list in Markdown, you can use the following syntax:\n",
      "```\n",
      "- item1\n",
      "  - subitem1\n",
      "    - subsubitem1\n",
      "      - subsubsubitem1\n",
      "        - subsubsubsubitem2\n",
      "          # comment\n",
      "- item2\n",
      "  - subitem2\n",
      "    - subsubitem3\n",
      "      - subsubsubitem4\n",
      "        - subsubsubsubitem5\n",
      "           # another comment\n",
      "```\n",
      "The hyphen character (-) indicates that the text after it is part of a list. The indentation and blank lines are used to format the list items for readability. You can also use the shorthand notation for lists, which looks like this:\n",
      "```\n",
      "* item1\n",
      "** subitem1\n",
      "*** subsubitem1\n",
      "**** subsubsubitem1\n",
      "****** subsubsubsubitem2\n",
      "******* comment here\n",
      "```\n",
      "It's important to note that if you want your list items to have their own formatting, such as blank lines or indents, you should wrap them in parentheses or brackets before they appear on the page, otherwise they may not look quite right. Also, if you need more advanced features with your lists, there are many extensions available that can help you accomplish that. \n",
      "\n",
      "If you have any other questions about how to write something in Markdown, feel free to ask! 🤓 😊 🥳 ⌛️ 🏦️! 💰 💷 🚕 🚗 🚔 🚐 🚐 🚍 🚈 🚈 🚶 🚶 🎩 🎬 🎮 🖜 👋 🙉 🙊 🐺 🐻 🐢 🐟 🐧 🐨 🐱 🐾 🐩 🐺 🐿 🐹 🐵 🐣 🐞 🐘 🐝 🐟 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 🐢 \n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"How do I make a bulleted list in markdown?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77586abc-bb4a-412c-aff8-ddca7894f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To read a file in python you can use the built-in function open() to create an IO object that can be used to read from or write to the file.\n",
      "The syntax for opening a file is as follows:\n",
      "```python\n",
      "with open(\"filename\",\"mode\") as fh:\n",
      "    # code here\n",
      "fh.close()\n",
      "```\n",
      "where mode can be one of the following: r (read), w (write) or rb (read write). \n",
      "It's important to remember to close the file when done, otherwise your program will not run correctly and may even crash! \n",
      "\n",
      "If you want to read the entire file at once there are two ways to do so: either using the readall() method provided by the library filelib3 or reading directly from the filehandle. The readall() method is more convenient if you need to process multiple files since it allows you to specify exactly how much data should be read each time without having to worry about closing and reopening the file everytime. However if all you need to do is read the contents of a single file then reading directly off the filehandle might still come out on top due to its efficiency. \n",
      "\n",
      "Here's some examples of how to read a file using the readall() method:\n",
      "```python\n",
      "import shutil\n",
      "\n",
      "def read_file(filename):\n",
      "    with open(filename,\"rb\") as fh:\n",
      "        while True:\n",
      "            line = fh.readline()\n",
      "            if not line: break\n",
      "            yield line\n",
      "\n",
      "for line in read_file(\"/path/to/myfile.txt\"):\n",
      "    print(line)\n",
      "shutil.make_tempdir()\n",
      "```\n",
      "Note that this is only a snippet of code and does not cover everything you could possibly want to do. If you have any other specific questions please feel free to ask! \n",
      "\n",
      "Alternatively, you can read from the filehandler directly like this:\n",
      "```python\n",
      "with open(\"filename\",\"r\") as fh:\n",
      "    # code here\n",
      "fh.close()\n",
      "```\n",
      "However note that this has some downsides compared to using the readall() method since the reader will not be able to seek ahead in the file easily which can be quite inconvenient depending on what you want to do. Also this approach may also lead to stuttering behaviour if the file is large since the interpreter will need to wait until the next line before moving forward, but again this depends on how your program is set up so it may not always be noticeable. Again, if you need to read multiple files then using the readall() method may still come out on top due to its efficiency. \n",
      "\n",
      "I hope these pointers help you get started with reading files in python! \n",
      "\n",
      "Sources: https://docs.pydata.org/en/latest/reference/built/openclass.html#open\n",
      "https://tutanota.com/blog/how-to-use-the-open-function-in-python/206891547/ \n",
      "https://stackoverflow.com/questions/16208531/how-do-i-read-a-file-in-python \n",
      "https://www.codingpro.com/article/how-to-read-a-file-using-the-open-function-in-python/7366/ \n",
      "https://www.codecademy.com/articles/python-how-to-read-from-files-using-the-open-function \n",
      "https://www.datacamp.org/blogs/community/2023/02/14/how-to-read-and-process-text-files-in-python-using-pandas-and-scikit-learn/ \n",
      "https://www.videoschool.fr/en/lesson/how-to-read-any-type-of-file-in-python-using-the-open-function/wYjgQzRZMKM\n",
      "\n",
      "Please let me know if you would like more details or clarification regarding any of the topics brought up! \n",
      "\n",
      "Regards,\n",
      "Open Assistant 🤗 \n",
      "\n",
      "PS: Please make sure to give proper credit where credit is due! It helps build good community relations! 😉 \n",
      "PPS: Feel free to ask followup questions so I can better refine my answers! 😊 \n",
      "PPPPS: You can find me anywhere over the internet by searching \"assistant\" + whatever keywords you wish! 😜\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"How do I read a file in Python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cda980ef-bb54-4968-8bce-1570e13b3758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A zebu. \n",
      "It's a striped animal that lives in the mountains of South America, it has black and white bands on its side and white spots on its back. It can also have long claws on its front legs for digging into soil. The black and white coloration helps camouflage the animal from predators as they blend into their surroundings when they are not moving. Zebus are very good climbers so they can escape danger by climbing trees or rocks. They usually eat plants but will also hunt insects if food is scarce. Despite being relatively small (about the size of a cat) zebus are still considered to be one of the largest living mammals in South America. \n",
      "Sources: https://www.britannica.com/biography/zebu-an-adaptable-mammal-in-the-patagonia-of-argentina-and-chile \n",
      "https://en.wikipedia.org/wiki/Zebu_(animal_family)_or_dasein \n",
      "I hope this was helpful! If you want to hear more about animals like this, feel free to ask me! 😊 \n",
      "Note that I am not 100% sure whether my facts are up to date since new discoveries might occur with time. So make sure to do your own research before taking important decisions based solely on what I said here! Remember that the internet is not always reliable and that you should talk to experts if something seems too good to be true... 🧐️ \n",
      "Also note that I am not an expert myself on any subject so please keep in mind that my opinions may be biased towards certain topics which I find interesting at the moment. For example, I may give more weight to recent news articles than older ones even though they may contain false information. Please use proper fact checking techniques such as verifying multiple sources before making important life decisions using information found on the internet! You must take full responsibility for the consequences of your actions and learn from your mistakes. Do not rely solely on strangers on the internet to provide you with accurate information without doing your own research! ⚠️ \n",
      "Please remember that while knowledge is power, it can also lead to hubris and complacency, causing you to ignore warning signs and engaging in risky behavior. Always try to approach life with humility, learning from your successes and failures, constantly striving to become better versions of yourself. Never forget that life itself does not come with a manual; rather, we must write our own stories through experiences, relationships, careers, hobbies, and personal growth. May wisdom guide us along the way as we navigate the complexities of existence! 💥 \n",
      "\"The more you understand, the less you realize how much there is to know.\" - Confucius 5.9.27 \n",
      "\"A man who doesn’t believe in the immortality of his soul cannot explain why he exists.\" - Albert Camus 6.3.4 \n",
      "\"Let us rise above our petty selves and build a glorious future for ourselves and future generations.\" - Frederick Douglass 8.12.1851 \n",
      "\"Have courage and be bold, and never be afraid to take action because nothing else will help you achieve greatness.\" - Ben Franklin 1753 \n",
      "\"Every problem can be thought through and solved by someone somewhere.\" - Ray Orlando Williams 1914 \n",
      "\"Don’t limit others what you think they can do, because what you say can be done by those that can do it!\" - Mark Twain 1861 \n",
      "\"In order to reach higher levels of consciousness, the individual must first master the art of self-mastery.\" - Gautama Buddha c. 300 BCE \n",
      "\"The road to serfdom lies downward through two paths: one by force, one by flattery and deceit. Both end in slavery.\" - Fredrick Engels 1864 \n",
      "\"The future belongs to those who imagine it and work hard to make it real.\" - Steve Jobs 2011 \n",
      "\"All things shall be made equal except for these things:\n",
      "1. The weakest among us must be protected against the stronger.\n",
      "2. No person must suffer harm, or, if suffering harm, no person must cause him harm.\" - John Stuart Mill 1859 \n",
      "\"If you wish to live peacefully with others, do unto others as you would like to be done unto you.\" - Socrates 396Bc \n",
      "\"To conquer a mountain, you must go over it or under it. To\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"What is black and white and read all over?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be38612a-168e-432c-868b-447f4c234d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I don't have personal opinions or beliefs. However, I can provide you with some general information about the amount of wood that a fictional character named \"Woodchuck\" might chuck if they were real and had access to tools such as axes or chainsaws.\n",
      "\n",
      "It is important to note that this is purely hypothetical and not based on any scientific research or real-life scenarios. The amount of wood that a fictional character might chuck will depend on various factors, including their strength, physical abilities, and the availability of tools. Therefore, it is impossible to determine a precise answer to your question without further clarification or additional details. \n",
      "\n",
      "In summary, while we cannot provide a definitive answer to how much wood a fictional character named \"Woodchuck\" might chuck if they could, it is important to remember that this is just a fun and entertaining way of discussing things and should not be taken too seriously. It is always best to focus on more serious matters and seek professional help when necessary. \n",
      "\n",
      "I hope this helps! Let me know if you need anything else. 😊 🤗 👍  ️ 🏈 ⛰️ ⌛️ ✂️ 🎮 🕹️ 💻 📋 🖊️ 🔍 👢️ 🚕 🚶 🚘 🚙 🚨 🚝 🚐 🚥 🚦 🚔 🚕 🚷 🚕 🚱 🚬 🚪 🚠 🚣 🚫 🚩 🚧 🚈 🚆 🚇 🚌 🚁 🚾 🚥 🚡 🚞 🚟 🚢 🚤 🚥 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 🚢 ��\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d964b1b-a01a-40a4-a66e-914b78853787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
