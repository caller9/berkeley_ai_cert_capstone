{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b35880-2789-4697-870b-2434551af7dd",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "This notebook will use the `data/fine_tune.jsonl` file to fine tune the raw Mistral 7B baseline model to perform better at Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f286eb2a-31ac-4421-a710-2f94dc2625e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 13:29:25.330478: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-09 13:29:25.356275: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-09 13:29:25.902414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24d4d9-f3f9-4f02-9523-762ecd37979c",
   "metadata": {},
   "source": [
    "## Configuring parameters for training\n",
    "Parts of this process are based on a combination of [Fine-Tune Your Own Llama 2 Model in a Colab Notebook - Maxime Labonne](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32) \n",
    "and [A Beginnerâ€™s Guide to Fine-Tuning Mistral 7B Instruct Model - Adithya S K](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe) with some fine tuning of parameters to run on a single 24GB VRAM Nvidia Cuda compatible GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e094088b-a0c9-4ff5-a2a7-317478044522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# LoRa\n",
    "new_model_dir = \"mistralai-lora\"\n",
    "\n",
    "# Merged model\n",
    "merged_model_dir = \"merged-fine-tuned\"\n",
    "\n",
    "# Fine tuning data path\n",
    "fine_tune_file = \"data/fine_tune.jsonl\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs if not -1)\n",
    "max_steps = -1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 100\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 100\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 512\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# define the device type\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea7007-b4b2-4600-beaf-7e3a48da2c3c",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a458f392-742b-47d2-b2ee-faa57b767626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad5e52b9dd54dbebc56cdcc09f876f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb372dc3-d870-4945-b70c-fa56cf1bcf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d26a0-64b4-41d2-9ebe-06cab24e5e2e",
   "metadata": {},
   "source": [
    "## Test out the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98ea8549-b0de-4678-bde3-3335d3fad8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Omit this step to save VRAM for training. Uncomment to test.\n",
    "# messages = [ \n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "# model_inputs = encodeds.to(device)\n",
    "\n",
    "# generated_ids = base_model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "# decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "# print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69007e89-5211-4568-b6c7-6b1c550793a7",
   "metadata": {},
   "source": [
    "## Load the training dataset\n",
    "\n",
    "The training dataset consists of entries formatted as recommended by [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format) in JSON Lines `jsonl` format.\n",
    "\n",
    "Example:\n",
    "```\n",
    "{text: \"<s>[INST] Something the user says [/INST] Desirable response from the model.</s>\"}\n",
    "{text: \"<s>[INST] Another thing from the user [/INST] Another response from the model.</s>\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f7782f-da0a-48c1-a280-3a364b136149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e2a036c0e54f39bd9846c71210ef34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3baecc14edb24909b7d82040089d197b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bb42ca05ed4ba895a15e13f954f4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset('json', data_files=fine_tune_file , split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874316a5-d8f8-46d6-9aaa-ad03a7977d91",
   "metadata": {},
   "source": [
    "## Configure and run fine tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c7a3e5-d952-4bb8-80e6-417e88f7be71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f6613797bf4ed08196fa7a5bdd8b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6cd63-72c1-4264-bb31-61bcbfc9fac9",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec676e89-3079-4a0e-99a0-b3ebb14cf79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 08:07, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.218500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee55ca-63db-4de0-9b85-dbc95e762528",
   "metadata": {},
   "source": [
    "## Free up VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b8e09c-c22f-4b62-9c11-813138e4d54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41285"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del base_model\n",
    "gc.collect()\n",
    "\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850969f9-28af-414f-a3cd-c39f71113ed8",
   "metadata": {},
   "source": [
    "Sometimes you'll need to run this cache clearing step multiple times if the following \"Save the merged model\" step causes an out of memory (OOM) error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3eb143-50b7-4b37-a1aa-a3758df69e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab255dd1-8362-4eb3-8e1b-d63464a10ccc",
   "metadata": {},
   "source": [
    "## Save the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83eaaff5-6a18-4fa3-a6ad-3559ee28e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915de42a83a44b30979846339de8a130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, new_model_dir)\n",
    "merged_model = merged_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d22961-218c-4333-82a2-35336aa325e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged model\n",
    "merged_model.save_pretrained(merged_model_dir,safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db30f0-3c2c-4666-a170-9143a23dec4a",
   "metadata": {},
   "source": [
    "## Test out the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91523e06-d678-4ca6-9d19-f809095cf375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_model(model, msg):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": msg},\n",
    "    ]\n",
    "    \n",
    "    encoded = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(encoded, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id, temperature=0.4, repetition_penalty=1.20)\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return decoded\n",
    "    \n",
    "def send_chat(model, msg):\n",
    "    result = send_to_model(model, msg)[0]\n",
    "    return result.rsplit(\" [/INST] \", 1)[1]\n",
    "\n",
    "def print_chat(model, msg):\n",
    "    print(send_chat(model, msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0bfe170-99fb-454b-8307-04d1f6699c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a bulleted list in Markdown, you can use the asterisk symbol ( * ). To begin the list, place the asterisk at the start of each line and add a space. For example:\n",
      "\n",
      "* Item 1\n",
      "* Item 2\n",
      "* Item 3\n",
      "\n",
      "To create a nested bullet list, put the asterisk on the left margin, like this:\n",
      "\n",
      "* Item 1\n",
      "    * Sub-Item 1\n",
      "    * Sub-Item 2\n",
      "        * Sub-Sub-Item 1\n",
      "        * Sub-Sub-Item 2\n",
      "\n",
      "You can continue to nest as many levels of sub-items as you need. \n",
      "\n",
      "You can also use the hyphen (-) character to create a dash list. The syntax is the same as for the asterisk. \n",
      "\n",
      "For example:\n",
      "\n",
      "- Item 1\n",
      "- Item 2\n",
      "- Item 3\n",
      "\n",
      "And if you want to create an ordered list, which is numbered instead of bullet-pointed, you can use the number sign (#). \n",
      "\n",
      "For example:\n",
      "\n",
      "# Item 1\n",
      "# Item 2\n",
      "# Item 3 \n",
      "\n",
      "Note that when using the number sign, you must leave one blank line between the numbers and the text. If there are no spaces, the numbers will not be counted. \n",
      "\n",
      "This is all simple and easy to remember, so don't worry about it! \n",
      "\n",
      "Hope this helps! \n",
      "\n",
      "Source: https://www.markdownguide.org/basic-syntax/#lists \n",
      "https://stackoverflow.com/questions/4597677/how-do-i-create-a-numbered-list-in-markdown \n",
      "https://www.freecodecamp.org/news/how-to-make-ordered-and-unordered-lists-in-markdown/ \n",
      "https://en.wikipedia.org/wiki/Markdown#Lists \n",
      "https://www.hackr.io/blog/how-to-write-a-markdown-document \n",
      "https://www.theodinproject.com/lessons/foundations-of-programming/markdown#lists \n",
      "https://www.w3schools.com/markdown/ \n",
      "https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax \n",
      "https://help.github.com/articles/basic-writing-and-formatting-syntax/ \n",
      "https://developer.atlassian.com/docs/confluence-markup/ \n",
      "https://www.markdownguide.org/extended-syntax/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/extended-syntax/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org/cheatsheet/ \n",
      "https://www.markdownguide.org\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"How do I make a bulleted list in markdown?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77586abc-bb4a-412c-aff8-ddca7894f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To read a file in python, you can use the open() function.\n",
      "\n",
      "```python\n",
      "file = open(\"filename\", \"r\")\n",
      "contents = file.read()\n",
      "print(contents)\n",
      "file.close()\n",
      "``` \n",
      "\n",
      "The first line opens the file with the given filename and mode (in this case, r means read-only). The second line reads all of the contents from the file into the variable contents. The third line prints out the contents to the console. The fourth line closes the file. \n",
      "\n",
      "To write a file in python, you can use the following code:\n",
      "\n",
      "```python\n",
      "file = open(\"filename\", \"w\")\n",
      "file.write(\"This is the content of the file.\")\n",
      "file.close()\n",
      "``` \n",
      "\n",
      "The first line opens the file with the given filename and mode (in this case, w means write-only). The second line writes the specified text to the file. The third line closes the file. \n",
      "\n",
      "Is there anything else you would like me to assist you with? \n",
      "\n",
      "Thank you! \n",
      "\n",
      "- Pybot ðŸ¤– \n",
      "\n",
      "P.S. If you have any other questions or need help, please let me know! I'm here to help. \n",
      "\n",
      "P.P.S. If you want to learn more about how to use the open() function, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.S. If you want to learn more about how to use the write() function, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.P.S. If you want to learn more about how to use the close() function, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.P.P.S. If you want to learn more about how to use the open() function with different modes, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.P.P.P.S. If you want to learn more about how to use the write() function with different modes, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.P.P.P.P.S. If you want to learn more about how to use the close() function with different modes, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.P.P.P.P.P.S. If you want to learn more about how to use the open() function with different modes, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.P.P.P.P.P.P.S. If you want to learn more about how to use the write() function with different modes, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.P.P.P.P.P.P.P.S. If you want to learn more about how to use the close() function with different modes, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.P.P.P.P.P.P.P.P.S. If you want to learn more about how to use the open() function with different modes, you can check out the official Python documentation at https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files \n",
      "\n",
      "P.P.P.P.P.P.P.P\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"How do I read a file in Python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cda980ef-bb54-4968-8bce-1570e13b3758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to your question is a newspaper. \n",
      "\n",
      "Newspapers are commonly printed in black ink on white paper, which makes the text easy to read. However, newspapers can also be printed in other colors and on different types of paper, such as newsprint or glossy paper.\n",
      "\n",
      "The phrase \"black and white and read all over\" is often used to describe something that has been simplified or made more basic. For example, you might say that a complex issue has been reduced to \"black and white\" by oversimplifying it. This means taking away the nuance and complexity of the issue, and only focusing on the most basic aspects.\n",
      "\n",
      "This could mean that the issue is being discussed in a way that ignores important details or context, which could lead to misunderstandings or misinformation. It's important to remember that not everything is always clear cut, and there may be shades of grey when it comes to certain issues.\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"What is black and white and read all over?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be38612a-168e-432c-868b-447f4c234d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% of the wood that it is able to get.\n",
      "\n",
      "Why? Because woodchucks are known for being very efficient and effective at what they do, and they have no reason to waste any energy on something that does not benefit them in some way.\n",
      "\n",
      "So it's possible that a woodchuck could chuck all of the wood that it gets, but it seems unlikely since it would be very difficult to find enough wood to accomplish this feat. \n",
      "\n",
      "If you want to know more about this topic, I can provide additional information. \n",
      "\n",
      "What do you think? Did you expect a different answer? \n",
      "\n",
      "Would you like me to explain my reasoning further? \n",
      "\n",
      "Is there anything else you would like me to help you with? \n",
      "\n",
      "Thank you! \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.S. If you are still unsure, please let me know and I will gladly explain my reasoning. \n",
      "\n",
      "I hope you enjoy your day! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.S. Please feel free to ask me any other questions you may have. \n",
      "\n",
      "I am happy to assist you in any way that I can. \n",
      "\n",
      "Thank you! \n",
      "\n",
      "Have a wonderful day! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.S. Is there anything else you need help with? \n",
      "\n",
      "Please don't hesitate to ask. \n",
      "\n",
      "I look forward to hearing from you soon. \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.S. I hope you have a fantastic day! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.S. Thank you for your time! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.P.S. Do you have any final questions? \n",
      "\n",
      "If so, I will be happy to answer them. \n",
      "\n",
      "Thank you! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.P.P.S. Please let me know if there is anything else that you require assistance with. \n",
      "\n",
      "I am here to help you. \n",
      "\n",
      "Thank you! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.P.P.P.S. I appreciate your business. \n",
      "\n",
      "Thank you for choosing us. \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.P.P.P.P.S. Have a wonderful day! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.P.P.P.P.P.S. Please let me know if there is anything else that you require assistance with. \n",
      "\n",
      "I am here to help you. \n",
      "\n",
      "Thank you! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.P.P.P.P.P.P.S. I appreciate your business. \n",
      "\n",
      "Thank you for choosing us. \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.P.P.P.P.P.P.P.S. Have a wonderful day! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.P.P.P.P.P.P.P.P.S. Please let me know if there is anything else that you require assistance with. \n",
      "\n",
      "I am here to help you. \n",
      "\n",
      "Thank you! \n",
      "\n",
      "Best regards, \n",
      "\n",
      "Your friend, \n",
      "\n",
      "The Woodchuck \n",
      "\n",
      "P.P.P.P.P.P.P.P.P.P.P.P.P.P.S. I appreciate\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d964b1b-a01a-40a4-a66e-914b78853787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
