{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b35880-2789-4697-870b-2434551af7dd",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "This notebook will use the `data/fine_tune.jsonl` file to fine tune the raw Mistral 7B instruct model to perform better at Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f286eb2a-31ac-4421-a710-2f94dc2625e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24d4d9-f3f9-4f02-9523-762ecd37979c",
   "metadata": {},
   "source": [
    "## Configuring parameters for training\n",
    "Parts of this process are based on a combination of [Fine-Tune Your Own Llama 2 Model in a Colab Notebook - Maxime Labonne](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32) \n",
    "and [A Beginnerâ€™s Guide to Fine-Tuning Mistral 7B Instruct Model - Adithya S K](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe) with some fine tuning of parameters to run on a single 24GB VRAM Nvidia Cuda compatible GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e094088b-a0c9-4ff5-a2a7-317478044522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# LoRa\n",
    "new_model_dir = \"mistralai-lora\"\n",
    "\n",
    "# Merged model\n",
    "merged_model_dir = \"merged-fine-tuned\"\n",
    "\n",
    "# Fine tuning data path\n",
    "fine_tune_file = \"data/fine_tune.jsonl\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 100\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# define the device type\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea7007-b4b2-4600-beaf-7e3a48da2c3c",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a458f392-742b-47d2-b2ee-faa57b767626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc45a9966e74eb88392c0125fcffec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb372dc3-d870-4945-b70c-fa56cf1bcf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d26a0-64b4-41d2-9ebe-06cab24e5e2e",
   "metadata": {},
   "source": [
    "## Test out the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db4b52e-e63d-4c7b-a607-66b177656423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pwnt/jupyter/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!  [INST] Do you have mayonnaise recipes? [/INST] Certainly! Here's a classic recipe for mayonnaise that you can easily whip up in your own kitchen:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 cup mayonnaise\n",
      "* 1 egg\n",
      "* 1 tablespoon Dijon mustard\n",
      "* 1 tablespoon apple cider vinegar\n",
      "* Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a small bowl, whisk together the egg, Dijon mustard, apple cider vinegar, and a pinch of salt and pepper until well combined.\n",
      "2. Slowly add the mayonnaise to the egg mixture, whisking constantly to incorporate it evenly.\n",
      "3. Once the mixture is smooth and creamy, transfer it to an airtight container and refrigerate for at least 30 minutes before serving.\n",
      "\n",
      "This mayonnaise recipe is sure to be a hit in your kitchen! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "generated_ids = base_model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69007e89-5211-4568-b6c7-6b1c550793a7",
   "metadata": {},
   "source": [
    "## Load the training dataset\n",
    "\n",
    "The training dataset consists of entries formatted as recommended by [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format) in JSON Lines `jsonl` format.\n",
    "\n",
    "Example:\n",
    "```\n",
    "{text: \"<s>[INST] Something the user says [/INST] Desirable response from the model.</s>\"}\n",
    "{text: \"<s>[INST] Another thing from the user [/INST] Another response from the model.</s>\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f7782f-da0a-48c1-a280-3a364b136149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f4f2ac8d9e4d38a033196973eee9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be759c941d9344fabfd95dfa07cd6b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f0dcce53a14f269906395ff75a0d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset('json', data_files=fine_tune_file , split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874316a5-d8f8-46d6-9aaa-ad03a7977d91",
   "metadata": {},
   "source": [
    "## Configure and run fine tuning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c7a3e5-d952-4bb8-80e6-417e88f7be71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pwnt/jupyter/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:173: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333aa8dddc5548a287bc7fb5d56df8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6cd63-72c1-4264-bb31-61bcbfc9fac9",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec676e89-3079-4a0e-99a0-b3ebb14cf79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:39, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee55ca-63db-4de0-9b85-dbc95e762528",
   "metadata": {},
   "source": [
    "## Free up VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b8e09c-c22f-4b62-9c11-813138e4d54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41285"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del base_model\n",
    "gc.collect()\n",
    "\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850969f9-28af-414f-a3cd-c39f71113ed8",
   "metadata": {},
   "source": [
    "Sometimes you'll need to run this cache clearing step multiple times if the following \"Save the merged model\" step causes an out of memory (OOM) error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3eb143-50b7-4b37-a1aa-a3758df69e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab255dd1-8362-4eb3-8e1b-d63464a10ccc",
   "metadata": {},
   "source": [
    "## Save the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83eaaff5-6a18-4fa3-a6ad-3559ee28e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69869f048ee74622ad7c35c9d247fbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, new_model_dir)\n",
    "merged_model = merged_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d22961-218c-4333-82a2-35336aa325e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged model\n",
    "merged_model.save_pretrained(merged_model_dir,safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db30f0-3c2c-4666-a170-9143a23dec4a",
   "metadata": {},
   "source": [
    "## Test out the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91523e06-d678-4ca6-9d19-f809095cf375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_model(model, msg):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": msg},\n",
    "    ]\n",
    "    \n",
    "    encoded = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(encoded, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id, temperature=0.9)\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return decoded\n",
    "    \n",
    "def send_chat(model, msg):\n",
    "    result = send_to_model(model, msg)[0]\n",
    "    return result.rsplit(\" [/INST] \", 1)[1]\n",
    "\n",
    "def print_chat(model, msg):\n",
    "    print(send_chat(model, msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0bfe170-99fb-454b-8307-04d1f6699c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make a bulleted list in markdown, optionally indent the line then use an asterisk at the beginning of each line. Example:\n",
      "* First list item \n",
      "* Second list item \n",
      "* Third list item \n",
      "  * Sub item under third item \n",
      "  * Another sub item under third item \n",
      "  * Last sub item under third item \n",
      "\n",
      "* Fourth list item \n",
      "* Fifth list item \n",
      "* Sixth list item \n",
      "  * Sub item under sixth item \n",
      "  * Another sub item under sixth item \n",
      "  * Last sub item under sixth item \n",
      "\n",
      "And so on.\n",
      "\n",
      "Note that in a bulleted list, the asterisk at the beginning of each line creates a bullet point. Sub items under a list item are indented one level further than the main item. This allows for nested sub items under a sub item. \n",
      "\n",
      "For example:\n",
      "* First list item \n",
      "  * First sub item under first item \n",
      "  * Second sub item under first item \n",
      "  * Third sub item under first item \n",
      "    * Sub sub item under third sub item \n",
      "    * Another sub sub item under third sub item \n",
      "    * Last sub sub item under third sub item \n",
      "  * Fourth sub item under first item \n",
      "  * Fifth sub item under first item \n",
      "\n",
      "This creates a bulleted list with five main items, each with their own sub items. First item has three sub items, second item has two sub items, third item has three sub items, fourth item has two sub items, and fifth item has two sub items. \n",
      "\n",
      "You can use this formatting to make bulleted lists in markdown.\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"How do I make a bulleted list in markdown?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77586abc-bb4a-412c-aff8-ddca7894f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To open a file in Python and read the entire contents into a string use this code: \n",
      "\n",
      "```\n",
      "with open('file.txt', 'r') as f:\n",
      "    contents = f.read()\n",
      "```\n",
      "\n",
      "Replace `'file.txt'` with the path of your file. The `'r'` mode tells Python to open the file in read mode. The contents of the file will be stored in the string `contents`. \n",
      "\n",
      "If you want to open the file in read mode and then close it after reading use this code: \n",
      "\n",
      "```\n",
      "with open('file.txt', 'r') as f:\n",
      "    contents = f.read()\n",
      "    f.close()\n",
      "```\n",
      "\n",
      "To open a file in Python and read the contents line by line use this code: \n",
      "\n",
      "```\n",
      "with open('file.txt', 'r') as f:\n",
      "    for line in f:\n",
      "        print(line.strip())\n",
      "```\n",
      "\n",
      "Replace `'file.txt'` with the path of your file. The `'r'` mode tells Python to open the file in read mode. The contents of the file will be read line by line and stored in the string `line`. The `strip()` method removes any leading or trailing white space from the line before printing it.\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"How do I read a file in Python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cda980ef-bb54-4968-8bce-1570e13b3758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is a newspaper. The reasoning behind this is that newspapers are typically printed in black and white, and the text is readable on the entire page.\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"What is black and white and read all over?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be38612a-168e-432c-868b-447f4c234d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of wood a woodchuck could chuck if a woodchuck could chuck wood is unknown. The phrase \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\" is a humorous question that does not have a logical answer because the hypothetical action of a woodchuck chucking wood is impossible.\n"
     ]
    }
   ],
   "source": [
    "print_chat(merged_model, \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d964b1b-a01a-40a4-a66e-914b78853787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
