{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da28ba98-c899-441a-9ea6-a78a7c7c9716",
   "metadata": {},
   "source": [
    "# Retrieval augmented chat - baseline\n",
    "\n",
    "This notebook is the primary demonstration of the project with the baseline model. Here we'll bring up the baseline model and vector database and start asking questions both with and without the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea286197-5a87-4f1b-abc3-bef2298a37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99fd829-3019-4de8-b27c-947c8a7f2d9b",
   "metadata": {},
   "source": [
    "## Initialize some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a05ed5f-66ef-4cc5-9813-736405bdefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"Corpus\"\n",
    "merged_model_dir = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "device_map = {\"\": 0}\n",
    "device = \"cuda\"\n",
    "database_top_n_results = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035b376-3b51-4da9-a4fd-23f2a7d2f5fc",
   "metadata": {},
   "source": [
    "## Load the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c10f83e-c140-4e54-8dd9-be410440bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"db/\")\n",
    "collection = client.get_collection(name = collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde5370-9aa9-4138-8e0e-a52e17c0f6db",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a41d92-5a28-4668-8e8b-2f37bf0180e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75edd0bfa0424c30a6d047c4f94874ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_model_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179fa16b-0172-45a1-b237-0d702a00e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_model(msg):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": msg},\n",
    "    ]\n",
    "    \n",
    "    encoded = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = merged_model.generate(encoded, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id, temperature=0.8)\n",
    "    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "def send_chat(msg):\n",
    "    result = send_to_model(msg)[0]\n",
    "    return result.rsplit(\" [/INST] \", 1)[1]\n",
    "\n",
    "def basic_chat(msg):\n",
    "    print(send_chat(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c18cc-2fd8-46b5-a107-1d21c494187a",
   "metadata": {},
   "source": [
    "## Trying the model without access to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551660eb-3f81-434e-8029-f0babd9b7969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To loop a GIF, you can use HTML5 and JavaScript. Here is an example code:\n",
      "```\n",
      "<img src=\"your-gif-url.gif\" alt=\"Your GIF\" loop>\n",
      "```\n",
      "This will loop the GIF indefinitely.\n",
      "\n",
      "If you want to add a pause and play button, you can use the following code:\n",
      "```\n",
      "<img src=\"your-gif-url.gif\" alt=\"Your GIF\" loop controls>\n",
      "```\n",
      "The `controls` attribute will add play and pause buttons to the GIF.\n"
     ]
    }
   ],
   "source": [
    "basic_chat(\"How do I loop a GIF?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0a712-94f8-4ed2-9049-9eb2df8bb674",
   "metadata": {},
   "source": [
    "## Add some retrieval convenience methods\n",
    "These methods use the vector database to find the `database_top_n_results` from the vector database, add them into the request context, then annotate the result with links to the documents used in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b632cef2-dec6-4f72-89e8-d09ee214dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "def retrieval_augmented_chat(msg):\n",
    "    query_result = collection.query(\n",
    "        query_texts=[msg], \n",
    "        n_results=database_top_n_results\n",
    "    )\n",
    "    question_with_context = \"\"\n",
    "    if len(query_result['documents'][0]) > 0:\n",
    "        question_with_context = \"Based on the following documents:\\n\" + \"\\n\\n\".join(query_result['documents'][0]) + \"\\n Answer the following question with lots of details: \"\n",
    "    question_with_context += msg\n",
    "    model_response = send_chat(question_with_context)\n",
    "\n",
    "    doc_links = \"\"\n",
    "    if len(query_result['metadatas'][0]) > 0:\n",
    "        doc_links = \"\\n\\n **Reference documents:** \\n\\n\"\n",
    "        for i in range(0, len(query_result['metadatas'][0])):\n",
    "            source = query_result['metadatas'][0][i]['source']\n",
    "            score = query_result['distances'][0][i]\n",
    "            doc_links += f\"* [{source}]({source}) score: {score:3.2f}\\n\"\n",
    "    return model_response + doc_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476ee89-d947-44e2-9c82-6797396cd3b2",
   "metadata": {},
   "source": [
    "## Run retrieval augmented chat\n",
    "Notice that the responses have switched from a general HTML/JavaScript context to include the document contents which provide examples of encoding the gif with ImageMagick so that it loops by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b0024a-5c2d-42bf-bbba-3533b3cf2fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To loop a GIF, you can use the `-loop` option in ImageMagick. The syntax is as follows:\n",
       "```css\n",
       "convert input.gif -loop count output.gif\n",
       "```\n",
       "Where `input.gif` is the name of the GIF file you want to loop, and `output.gif` is the name of the output file where you want to save the looped GIF. The `count` parameter specifies how many times you want the GIF to loop. For example, to loop the GIF indefinitely, you can set the count to `0`.\n",
       "\n",
       "Here's an example command to loop a GIF called `chrome-samesite-missing.gif` indefinitely and save the output file as `chrome-samesite-missing-loop.gif`:\n",
       "```bash\n",
       "convert chrome-samesite-missing.gif -loop 0 chrome-samesite-missing-loop.gif\n",
       "```\n",
       "It's important to note that the output filename should come after the `-loop` option. If you specify the filename before the `-loop` option, it will not work.\n",
       "\n",
       "You can also specify the number of times the GIF should loop using the `count` parameter. For example, to loop the GIF 10 times, you would use the following command:\n",
       "```bash\n",
       "convert chrome-samesite-missing.gif -loop 10 chrome-samesite-missing-loop.gif\n",
       "```\n",
       "In summary, to loop a GIF using ImageMagick, you need to specify the `-loop` option followed by the number of times you want the GIF to loop, and save the output file with the desired name.\n",
       "\n",
       " **Reference documents:** \n",
       "\n",
       "* [corpus/imagemagick/set-a-gif-to-loop.md](corpus/imagemagick/set-a-gif-to-loop.md) score: 0.97\n",
       "* [corpus/imagemagick/compress-animated-gif.md](corpus/imagemagick/compress-animated-gif.md) score: 1.28\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(retrieval_augmented_chat(\"How do I loop a GIF?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd178ea-8f8d-4a0f-a4ce-9bcde8dffd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, you can use `npx` with GitHub Actions. The `npx` command can be used to run a specific version of a npm package without having the package listed in the `package.json` file. This can be useful in cases where you want to use a specific version of a package for a particular task, or where you want to use a package that is not in the `package.json` file.\n",
       "\n",
       "To use `npx` with GitHub Actions, you will need to configure the `actions/cache` action to cache the package version that you want to use. This can be done by setting the `npm-tag` or `cache-dependency-key` option in the `actions/cache` action.\n",
       "\n",
       "For example, if you want to use `npx` to install the `get-graphql-schema` package from npm, you can configure the `actions/cache` action like this:\n",
       "```yaml\n",
       "- uses: actions/cache@v2\n",
       "  with:\n",
       "    path: /path/to/npm-dependencies\n",
       "    key: ${{ runner.os }}-npx-get-graphql-schema\n",
       "    restore-keys: |\n",
       "      ${{ runner.os }}-npm-dependencies/*\n",
       "    stage: cache\n",
       "    action: \"npm install\"\n",
       "    npm-tag: global\n",
       "```\n",
       "This will cache the `npx` installation of `get-graphql-schema` in the `npm-dependencies` directory. The `key` option is used to identify the package and the `npm-tag` option is used to specify that the package should be installed globally.\n",
       "\n",
       "Once the package is cached, you can use the `npx` command to run the `get-graphql-schema` package in your GitHub Actions workflow. For example:\n",
       "```yaml\n",
       "- name: Install and run get-graphql-schema\n",
       "  run: npx get-graphql-schema https://api.fly.io/graphql > flyctl/fly.graphql\n",
       "```\n",
       "This will run the `npx` command to install and run the `get-graphql-schema` package, using the version that is cached in the `npm-dependencies` directory.\n",
       "\n",
       "Overall, using `npx` with GitHub Actions can be a useful way to run specific versions of packages without having to manage dependencies in a `package.json` file. However, it is important to configure the `actions/cache` action correctly to ensure that the correct version of the package is used and that the cache is managed properly.\n",
       "\n",
       " **Reference documents:** \n",
       "\n",
       "* [corpus/github-actions/npm-cache-with-npx-no-package.md](corpus/github-actions/npm-cache-with-npx-no-package.md) score: 0.88\n",
       "* [corpus/github-actions/attach-generated-file-to-release.md](corpus/github-actions/attach-generated-file-to-release.md) score: 1.11\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(retrieval_augmented_chat(\"Can I use npx with GitHub actions?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3ed6c-25c5-43b4-a3fb-b7dc52dba8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
